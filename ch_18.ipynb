{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18 迴歸分析\n",
    "\n",
    "變異數分析可以讓我們知道一個離散變數對一個連續型的因變數有沒有影響。但是，對於兩個連續型變數，一個連續型的變數對另一個連續型變數有沒有影響；有影響的話，這個影響具體有多大；當一個連續型變數變化時，另一個連續型變數會變多少等問題，直接使用變異數分析無法給我們提供參考答案。若要探討這個問題，則需進行迴歸分析（Regression Analysis）。迴歸分析能夠將多個連續型或是離散型的變數之間的關係量化，進而找到事物之間的線性相關關係或是進行預測。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.1 一元線性迴歸模型\n",
    "\n",
    "#### 18.1.1 一元線性迴歸模型\n",
    "\n",
    "假設有兩個變數：自變數（Independent Variable）$X$ 和因變數（Dependent Variable）$Y$，一元線性迴歸模型（Simple Linear Regression Model）是指用線性模型 $\\alpha+\\beta X$ 來刻畫 $Y$ 的變動，其中 $\\alpha$ 和 $\\beta$ 是未知的、待估計的參數。通常當變數有隨機的成分在時，就不能用一個變數完全解釋另一個變數，因此我們將 $Y$ 表達為：\n",
    "\n",
    "$$Y_i=\\alpha+\\beta X_i+\\varepsilon_i,\\ i=1,2,\\cdots,n,$$\n",
    "\n",
    "其中\n",
    "* $Y_i$ 為變數 $Y$ 的第 $i$ 個樣本，$X_i$ 為變數 $X$ 的第 $i$ 個樣本，共有 $n$ 個樣本，其中 $X_i$ 取值不能全部相等，也就是 $X$ 為非常數，假設所有 $X$ 的取值都相等，則可以推斷 $Y$ 的變化肯定不是由 $X$ 引起的，研究 $X$ 對 $Y$ 的影響毫無意義。\n",
    "\n",
    "* $\\varepsilon_i$ 為隨機誤差項（Error）或干擾項，表示 $Y_i$ 的變化中未被 $X_i$ 解釋的部分。\n",
    "\n",
    "* 參數 $\\alpha$ 和 $\\beta$ 都被稱為迴歸係數（Regression Coefficient），其中 $\\alpha$ 是截距項（Intercept）；$\\beta$ 為斜率項（Slope），表示 $X$ 每增加一個單位，$Y$ 平均會增加 $\\beta$ 的單位。\n",
    "\n",
    "這裡要注意的是，線性迴歸模型中的「線性」是指模型是參數的線性函數，並不要求變數 $X$ 和 $Y$ 的線性函數，例如我們認為 $X$ 與 $Y$ 滿足\n",
    "\n",
    "$$Y_i=\\alpha+\\beta\\ln X_i+\\varepsilon_i$$\n",
    "\n",
    "該模型可以轉化為線性模型：\n",
    "\n",
    "$$Y_i=\\alpha+\\beta Z_i+\\varepsilon_i$$\n",
    "\n",
    "其中 $Z_i=\\ln X_i$ 可以透過對樣本 $X_i$ 取對數得到。因此只要是參數的線性模型，都適用於本章的分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18.1.2 最小平方法\n",
    "\n",
    "在上述的線性模型中，$X_i$ 和 $Y_i$ 是樣本數據，但還需知道 $\\alpha$ 與 $\\beta$ 的值。對於 $\\alpha$ 與 $\\beta$ 的值，可以根據 $X_i$ 和 $Y_i$ 的數據來估計。估計的方法有很多種，例如前面講過的動差估計法就是一種。但是，很明顯的，採取不同的估計方法，就會得到不同的 $\\alpha$ 與 $\\beta$ 的估計值。而我們想要的自然是最接近真實值的估計。那麼，應該採取什麼方法進行估計呢？建構線性迴歸模型的目的是要來解釋 $Y$ 的變動，因此我們認為 $\\alpha$ 與 $\\beta$ 的估計式是令 $Y_i$ 中未被 $X_i$ 解釋的部分 $\\varepsilon_i$ 越小越好。我們可以將誤差項 $\\varepsilon_i$ 加總令其最小，但是，誤差 $\\varepsilon_i$ 有時大於零有時小於零，加總時會正負抵消，這樣就無法區分沒有差異和正負差異相抵消時的情況。因此，我們選擇使用誤差 $\\varepsilon_i$ 之平方和並令其最小，這種估計方法被稱為最小平方法（Least Squares Method）。求解 $\\alpha$ 和 $\\beta$ 的方式可以用數學是表達如下：\n",
    "\n",
    "$$\\min_{\\alpha,\\beta}Q\\big(\\alpha,\\beta\\big)=\\frac{1}{n}\\sum_{i=1}^n\\big(Y_i-\\alpha-\\beta X_i\\big)^2$$\n",
    "\n",
    "根據函數最小化原則，能夠令誤差平方和 $Q\\big(\\alpha,\\beta\\big)$ 最小的 $\\alpha、\\beta$必須滿足以下一階條件（First Order Conditions）：\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial Q\\big(\\alpha,\\beta\\big)}{\\partial\\alpha}&=-2\\frac{1}{n}\\sum_{i=1}^{n}\\big(Y_i-\\alpha-\\beta X_i\\big)=0 \\\\\n",
    "\\frac{\\partial Q\\big(\\alpha,\\beta\\big)}{\\partial\\beta}&=-2\\frac{1}{n}\\sum_{i=1}^{n}\\big(Y_i-\\alpha-\\beta X_i\\big)X_i=0\n",
    "\\end{split} \\label{eq:18.1}\\tag{18.1}\\\\\n",
    "$$\n",
    "\n",
    "根據這兩條一階條件式，可以得到 $\\alpha$ 與 $\\beta$ 的最小平方估計式（Estimator）：\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\hat\\beta&=\\frac{\\sum_{i=1}^{n}\\big(X_i-\\overline{X}\\big)\\big(Y-\\overline{Y}\\big)}{\\sum_{i=1}^{n}\\big(X_i-\\overline{X}\\big)^2}\\\\\n",
    "\\hat\\alpha&=\\overline{Y}-\\hat\\beta\\overline{X}\n",
    "\\end{split} \\label{eq:18.2}\\tag{18.2}\n",
    "$$\n",
    "\n",
    "其中 $\\overline{X}$ 和 $\\overline{Y}$ 為 $X$ 和 $Y$ 的樣本均值，注意式 (\\ref{eq:18.2}) 是估計式，也就是說，樣本觀測值不同，我們就會得到不同的 $\\hat\\alpha、\\hat\\beta$。如果我們將樣本觀測值 $x_1,x_2,...,x_n$ 和 $y_1,y_2,...,y_n$ 代入估計式 (\\ref{eq:18.2}) 中，即可得到 $\\hat\\alpha$ 與 $\\hat\\beta$ 之最小平方估計值（Estimate）：\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\hat\\beta_n&=\\frac{\\sum_{i=1}^n\\big(x_i-\\overline{x}\\big)\\big(y_i-\\overline{y}\\big)}{\\sum_{i=1}^{n}\\big(x_i-\\overline{x}\\big)^2}\\\\\n",
    "\\hat\\alpha_n&=\\overline{y}-\\hat\\beta\\overline{x}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "其中 $\\overline{x}、\\overline{y}$ 是 $\\overline{X}、\\overline{Y}$的實現值。\n",
    "\n",
    "將 $\\hat\\alpha$ 與 $\\hat\\beta$ 之最小平方估計值代入線性模型，可得由樣本觀測值 $x_1,x_2,...,x_n$ 解釋的 $\\hat{y}_i$：\n",
    "\n",
    "$$\\hat{y}_i=\\hat\\alpha_n+\\hat\\beta_nx_i,\\ i=1,...,n$$\n",
    "\n",
    "這個 $\\hat{y}_i$ 被稱為擬和值（Fitted Value），而樣本觀測值 $y_i$ 與擬和值 $\\hat{y}_i$ 之間的差值為殘差值（Residual Value）：\n",
    "\n",
    "$$\\hat\\varepsilon_i=y_i-\\hat{y}_i,\\ i=1,...,n.$$\n",
    "\n",
    "另外根據一階條件式(\\ref{eq:18.1})，可得：\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\sum_{i=1}^{n}\\big(Y_i-\\hat\\alpha-\\hat\\beta X_i\\big)&=\\sum_{i=1}^{n}\\hat\\varepsilon_i=0\\\\\n",
    "\\sum_{i=1}^{n}\\big(Y_i-\\hat\\alpha-\\hat\\beta X_i\\big)X_i&=\\sum_{i=1}^{n}\\hat\\varepsilon_iX_i=0\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.2 模型擬合度\n",
    "\n",
    "在現實中，可能不同的 $X$ 都可以用來解釋 $Y$ 之變動，例如自變數 $Y$ 是股票收益率，有人可能用公司每股收益解釋之，有人可能用公司市值解釋之。因此，我們需要一個指標能夠判斷出模型的好壞，對此，我們引入擬合度（Goodness of Fit）的概念。擬合度是指迴歸直線與樣本數據趨勢的吻合程度，取決於估計方法和樣本數據。\n",
    "\n",
    "常用來判斷線性模型擬合度的指標是 $R^2$（R Square），又稱作可決係數（Coefficient of Determination），它刻畫的是自變數與因變數關係密切的程度，由迴歸模型解釋的變動量占 $Y$ 總變動量的百分比來刻畫。如果將因變數 $Y$ 的波動（變異）進行分解，可以得到如下式子：\n",
    "\n",
    "$$\\sum_{i=1}^{n}\\big(Y_i-\\overline{Y}\\big)^2=\\sum_{i=1}^{n}\\big(\\hat{Y}_i-\\overline{Y}\\big)^2+\\sum_{i=1}^{n}\\hat\\varepsilon^2$$\n",
    "\n",
    "其中\n",
    "* 等號左邊為總平方和（Total Sum of Squares，TSS）：度量了因變數 $Y_i$ 的總波動情況；\n",
    "\n",
    "* 等號右邊第一項為迴歸平方和（Regression Sum of Squares，RSS）：度量了模型估計出來的 $\\hat{Y}_i$ 的波動情況，由於 $\\hat{Y}_i$ 是根據自變數 $X_i$ 估計的，所以這一部分可以說是總平方和中可以被自變數 $X_i$ 解釋的波動；\n",
    "\n",
    "* 等號右邊第二項為殘差平方和（Error Sum of Squares，ESS）：度量了殘差的波動，即不能被自變數 $X_i$ 解釋的波動。\n",
    "\n",
    "由於 TSS、RSS、ESS 均為平方和，三者的關係為：\n",
    "\n",
    "$$T\\!S\\!S=R\\!S\\!S+E\\!S\\!S \\label{eq:18.3}\\tag{18.3}$$\n",
    "\n",
    "$R^2$ 的表達是為\n",
    "\n",
    "$$R^2=\\frac{R\\!S\\!S}{T\\!S\\!S}$$\n",
    "\n",
    "根據式 (\\ref{eq:18.3}) 可知 $R^2$ 在 0 到 1 之間取值，其值越大說明可用模型解釋的波動部分占總波動比例越高，亦即預測值（$\\hat{y}_i$）越接近觀測值（$y_i$）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.3 古典假設條件下 $\\hat\\alpha、\\hat\\beta$ 之統計性質\n",
    "\n",
    "最小平方法是利用誤差平方和最小得出參數 $\\alpha$ 與 $\\beta$ 的估計式，該估計式是樣本 $X_i$ 和 $Y_i$ 的函數，也就是說不同的樣本觀測值會有不同的參數估計值 $\\hat\\alpha、\\hat\\beta$。一般來說，隨機抽樣導致的波動和選擇的估計方法不同會使參數估計值與參數的真實值存在差距，因此 $Y_i$ 和 $X_i$ 需要滿足一些隨機條件，以衡量估計式之統計性質。以下為 $Y_i$ 和 $X_i$ 需要滿足的假設條件：\n",
    "\n",
    "1. $X_i, i=1,2,...,n$ 為非隨機的變數。\n",
    "\n",
    "2. 存在參數的真實值 $\\alpha_0、\\beta_0$ 使得 $Y_i=\\alpha_0+\\beta_0X_i+e_i, i=1,2,...,n$，其中干擾項 $e_i$ 滿足以下性質：\n",
    "\n",
    " 1. $\\text{E}(e_i)=0,\\ i=1,2,...,n$，基於這個條件及 $X_i$ 非隨機的假設，可以得到 $\\text{E}(Y_i)=\\alpha_0+\\beta_0X_i$。\n",
    " \n",
    " 2. $\\text{Var}(e_i)=\\sigma_0^2,\\ i=1,2,...,n$，這個條件又被稱為變異數同質性（Homoscedasticity）條件；當 $i\\ne j$ 時，$\\text{Cov}(e_i,e_j)=0$，這個條件式要求 $e_i$ 無自相關性。在最小平方法的框架下，干擾項 $e_i$ 之變異數 $\\sigma_0^2$ 的估計式為：\n",
    " \n",
    " $$\\hat\\sigam^2=\\frac{1}{n-2}\\sum_{i=1}^{n}\\hat\\varepsilon_i^2$$\n",
    " \n",
    "以上的假設條件被稱為迴歸的古典條件（Classical Conditions）。\n",
    "\n",
    "接下來我們來分析最小平方估計式 $\\hat\\alpha、\\hat\\beta$ 和 $\\hat\\varepsilon^2$ 的統計性質。在古典假設條件下，可以證明最小平方估計式 $\\hat\\alpha、\\hat\\beta$ 是 $\\alpha_0、\\beta_0$ 的最佳線性無偏估計式（Best Linear Unbiased Estimator），其中「線性」是指 $\\hat\\alpha、\\hat\\beta$ 為所有 $Y_i$ 之線性組合；「無偏」是指 $\\text{E}(\\hat\\alpha)=\\alpha_0、\\text{E}(\\hat\\beta)=\\beta_0$；「最佳」是指任意其他線性無偏估計式 $\\breve\\alpha、\\breve\\beta$ 之變異數均大於 $\\hat\\alpha、\\hat\\beta$ 的變異數，也就是說對於所有的線性無偏估計式 $\\hat\\alpha、\\hat\\beta$，$\\text{Var}(\\breve\\alpha)\\ge\\text{Var}(\\hat\\alpha)、\\text{Var}(\\breve\\beta)\\ge\\text{Var}(\\hat\\beta)$。\n",
    "\n",
    "此外，我們還可以得到估計式 $\\hat\\alpha、\\hat\\beta$ 的概率分佈。當滿足古典線性迴歸模型假設時，最小二乘估計量 $\\hat\\alpha、\\hat\\beta$ 服從常態分佈：\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\hat\\alpha &\\sim N\\left(\\alpha_0,\\sigma_0^2\\left(\\frac{1}{n}+\\frac{\\overline{x}^2}{\\sum_{i=1}^{n}\\big(X_i-\\overline{X}\\big)^2}\\right)\\right)\\\\\n",
    "\\hat\\beta  &\\sim N\\left(\\beta_0,\\sigma_0^2\\left(\\frac{1}{\\sum_{i=1}^{n}\\big(X_i-\\overline{X}\\big)^2}\\right)\\right)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "另外，干擾項 $e_i$ 之變異數 $\\sigma_0^2$ 的估計式 $\\sigma_0^2=\\frac{1}{n-2}\\sum_{i=1}^{n}\\hat\\varepsilon_i^2$ 滿足以下分佈：\n",
    "\n",
    "$$\\frac{\\big(n-2\\big)\\hat\\sigma^2}{\\sigma_0^2}\\sim\\chi^2\\big(n-2\\big)$$\n",
    "\n",
    "從 $\\hat\\alpha、\\hat\\beta$ 的均值和變異數可以看出，$\\hat\\alpha、\\hat\\beta$ 具有以下性質：\n",
    "\n",
    "1. 隨著樣本量 $n$ 的增加，估計式的精確度也會隨著增加。每增加一個觀測樣本點，模型的精確度將會提高。\n",
    "\n",
    "2. $\\sum_{i=1}^{n}\\big(X_i-\\overline{X}\\big)^2$ 在 $\\hat\\alpha、\\hat\\beta$ 的變異數估計式中都有出現、且都在分母當中，說明 $\\sum_{i=1}^{n}\\big(X_i-\\overline{X}\\big)^2$ 越大，$\\hat\\alpha、\\hat\\beta$ 變異數越小，估計式的精確度增加。$\\sum_{i=1}^{n}\\big(X_i-\\overline{X}\\big)^2$ 表示 $X$ 變數樣本與樣本的離散程度。也就是說當樣本在 $X$ 取值上越離散，估計式的變異數越小；在 $X$ 取值上越集中時，變異數越大。\n",
    "\n",
    "3. $\\overline{X}^2$ 代表數據點相對於 $y$ 軸的平均偏離程度，當 $\\overline{X}$ 離 $y$ 軸越遠，迴歸估計線的截距項 $\\hat\\alpha$ 越不容易確定，$\\hat\\alpha$ 的精確度就會隨之降低。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.4 顯著性檢定\n",
    "\n",
    "回想一下前面章節介紹的假設檢定，對於一個隨機變數，可以對其期望是否等於某個值進行檢定。在迴歸分析中，我們往往會對估計出的係數進行假設檢定。通常我們是對迴歸係數的真實值 $\\alpha_0、\\beta_0$ 提出虛無假設，然後計算統計量之取值，並根據 $\\hat\\alpha、\\hat\\beta$ 之分佈性質判斷根據樣本觀測點 $x_i,y_i$ 計算出來的 $\\hat\\alpha_n、\\hat\\beta_n$ 是否顯著異於虛無假設之取值，以此來判斷是否接收虛無假設。也正因為如此，我們把這樣的假設檢定叫做顯著性檢定，即變數之間的關係是否顯著。以係數 $\\hat\\beta$ 為例，顯著性檢定分為以下 4 個步驟：\n",
    "\n",
    "1. 提出虛無假設 $H_0:\\beta_0=b$；對立假設 $H_1:\\beta_0\\ne b$；\n",
    "\n",
    "2. 確定顯著性水平 0.05、0.01 等（根據具體情況而定）；\n",
    "\n",
    "3. 建構、計算統計量 $t_\\beta$－－標準化的 $\\hat\\beta-b$：\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "t_\\beta&=\\frac{\\big(\\hat\\beta-b\\big)}{\\left(\\frac{\\hat\\sigma}{\\sqrt{\\sum_{i=1}^{n}\\big(X_i-\\overline{X}\\big)^2}}\\right)}\\\\\n",
    "&=\\frac{\\sqrt{\\sum_{i=1}^{n}\\big(X_i-\\overline{X}\\big)^2}\\left(\\frac{\\hat\\beta-b}{\\sigma_0}\\right)}{\\sqrt{\\frac{(n-2)\\hat\\sigma^2}{\\sigma_0^2}\\times\\big(n-2\\big)}}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "式中 $\\sqrt{\\sum_{i=1}^{n}\\big(X_i-\\overline{X}\\big)^2}\\left(\\cfrac{\\hat\\beta-b}{\\sigma_0}\\right)\\sim N\\big(0,1\\big),\\cfrac{\\big(n-2\\big)\\hat\\sigma^2}{\\sigma_0^2}\\sim\\chi^2\\big(n-2\\big)$，由此可得 $t_\\beta$ 服從 $t$ 分佈：\n",
    "\n",
    "$$t_\\beta\\sim t\\big(n-2\\big)$$\n",
    "\n",
    "4. 查自由度 $df=n-2$ 的 $t$ 分佈表得到臨界值或是計算出 $t_\\beta$ 對應的 $p$ 值，作出是否拒絕或接受虛無假設的結論。例如，虛無假設中的 b 為 0 的情況下，如果接受 $\\beta_0=0$ 的虛無假設，說明變數 $X$ 對於變數 $Y$ 的影響係數為 0，則此模型的構造沒有太大意義；若拒絕 $\\beta_0=0$ 的虛無假設，說明變數 $X$ 對於變數 $Y$ 有顯著不為 0 的影響，則可根據 $X$ 的觀測值來預測隨機變數 $Y$。對於截距 $\\hat\\alpha$，我們可以採取同樣的方式進行檢定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.5 台灣加權指數與台灣 50 指數的迴歸分析與 Python 實踐\n",
    "\n",
    "由前面的章節分析得知，台灣加權指數與台灣 50 指數的日度收益率數據存在著相關性關係。在本章節中，對兩個指數的日度收益率進行一元線性迴歸分析，進一步確定二者的相關關係。在分析之前，我們先來瞭解 Python 中的一些迴歸函數。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18.5.1 Python 估計迴歸模型"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
