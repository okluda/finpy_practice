{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18 迴歸分析\n",
    "\n",
    "變異數分析可以讓我們知道一個離散變數對一個連續型的因變數有沒有影響。但是，對於兩個連續型變數，一個連續型的變數對另一個連續型變數有沒有影響；有影響的話，這個影響具體有多大；當一個連續型變數變化時，另一個連續型變數會變多少等問題，直接使用變異數分析無法給我們提供參考答案。若要探討這個問題，則需進行迴歸分析（Regression Analysis）。迴歸分析能夠將多個連續型或是離散型的變數之間的關係量化，進而找到事物之間的線性相關關係或是進行預測。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.1 一元線性迴歸模型\n",
    "\n",
    "#### 18.1.1 一元線性迴歸模型\n",
    "\n",
    "假設有兩個變數：自變數（Independent Variable）$X$ 和因變數（Dependent Variable）$Y$，一元線性迴歸模型（Simple Linear Regression Model）是指用線性模型 $\\alpha+\\beta X$ 來刻畫 $Y$ 的變動，其中 $\\alpha$ 和 $\\beta$ 是未知的、待估計的參數。通常當變數有隨機的成分在時，就不能用一個變數完全解釋另一個變數，因此我們將 $Y$ 表達為：\n",
    "\n",
    "$$Y_i=\\alpha+\\beta X_i+\\varepsilon_i,\\ i=1,2,\\cdots,n,$$\n",
    "\n",
    "其中\n",
    "* $Y_i$ 為變數 $Y$ 的第 $i$ 個樣本，$X_i$ 為變數 $X$ 的第 $i$ 個樣本，共有 $n$ 個樣本，其中 $X_i$ 取值不能全部相等，也就是 $X$ 為非常數，假設所有 $X$ 的取值都相等，則可以推斷 $Y$ 的變化肯定不是由 $X$ 引起的，研究 $X$ 對 $Y$ 的影響毫無意義。\n",
    "\n",
    "* $\\varepsilon_i$ 為隨機誤差項（Error）或干擾項，表示 $Y_i$ 的變化中未被 $X_i$ 解釋的部分。\n",
    "\n",
    "* 參數 $\\alpha$ 和 $\\beta$ 都被稱為迴歸係數（Regression Coefficient），其中 $\\alpha$ 是截距項（Intercept）；$\\beta$ 為斜率項（Slope），表示 $X$ 每增加一個單位，$Y$ 平均會增加 $\\beta$ 的單位。\n",
    "\n",
    "這裡要注意的是，線性迴歸模型中的「線性」是指模型是參數的線性函數，並不要求變數 $X$ 和 $Y$ 的線性函數，例如我們認為 $X$ 與 $Y$ 滿足\n",
    "\n",
    "$$Y_i=\\alpha+\\beta\\ln X_i+\\varepsilon_i$$\n",
    "\n",
    "該模型可以轉化為線性模型：\n",
    "\n",
    "$$Y_i=\\alpha+\\beta Z_i+\\varepsilon_i$$\n",
    "\n",
    "其中 $Z_i=\\ln X_i$ 可以透過對樣本 $X_i$ 取對數得到。因此只要是參數的線性模型，都適用於本章的分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18.1.2 最小平方法\n",
    "\n",
    "在上述的線性模型中，$X_i$ 和 $Y_i$ 是樣本數據，但還需知道 $\\alpha$ 與 $\\beta$ 的值。對於 $\\alpha$ 與 $\\beta$ 的值，可以根據 $X_i$ 和 $Y_i$ 的數據來估計。估計的方法有很多種，例如前面講過的動差估計法就是一種。但是，很明顯的，採取不同的估計方法，就會得到不同的 $\\alpha$ 與 $\\beta$ 的估計值。而我們想要的自然是最接近真實值的估計。那麼，應該採取什麼方法進行估計呢？建構線性迴歸模型的目的是要來解釋 $Y$ 的變動，因此我們認為 $\\alpha$ 與 $\\beta$ 的估計式是令 $Y_i$ 中未被 $X_i$ 解釋的部分 $\\varepsilon_i$ 越小越好。我們可以將誤差項 $\\varepsilon_i$ 加總令其最小，但是，誤差 $\\varepsilon_i$ 有時大於零有時小於零，加總時會正負抵消，這樣就無法區分沒有差異和正負差異相抵消時的情況。因此，我們選擇使用誤差 $\\varepsilon_i$ 之平方和並令其最小，這種估計方法被稱為最小平方法（Least Squares Method）。求解 $\\alpha$ 和 $\\beta$ 的方式可以用數學是表達如下：\n",
    "\n",
    "$$\\min_{\\alpha,\\beta}Q\\big(\\alpha,\\beta\\big)=\\frac{1}{n}\\sum_{i=1}^n\\big(Y_i-\\alpha_i-\\beta_iX_i\\big)^2$$\n",
    "\n",
    "根據函數最小化原則，能夠令誤差平方和 $Q\\big(\\alpha,\\beta\\big)$ 最小的 $\\alpha、\\beta$必須滿足以下一階條件（First Order Conditions）：\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial Q\\big(\\alpha,\\beta\\big)}{\\partial\\alpha}&=-2\\frac{1}{n}\\sum_{i=1}^{n}\\big(Y_i-\\alpha_i-\\beta_iX_i\\big)=0 \\\\\n",
    "\\frac{\\partial Q\\big(\\alpha,\\beta\\big)}{\\partial\\beta}&=-2\\frac{1}{n}\\sum_{i=1}^{n}\\big(Y_i-\\alpha_i-\\beta_iX_i\\big)X_i=0\n",
    "\\end{split} \\label{eq:18.1}\\tag{18.1}\\\\\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
